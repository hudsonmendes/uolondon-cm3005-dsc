{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Assignment: Building a Regression Model\n",
    "\n",
    "```\n",
    "University of London\n",
    "BSc in Computer Science\n",
    "CM3005, Data Science\n",
    "Hudson Leonardo MENDES\n",
    "hlm12@student.london.ac.uk\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-specific area\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "data_folderpath = pathlib.Path(\"./data\")\n",
    "\n",
    "ppd_folderpath = data_folderpath / \"uk-ppd\"\n",
    "inflation_filepath = data_folderpath / \"uk-ons/ons-inflation-1989-2022.csv\"\n",
    "interest_filepath = data_folderpath / \"uk-boe/boe-interest-1975-2022.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def build_count_properties_sold(ix: pd.DataFrame, n_days: int) -> Callable[[pd.DataFrame], int]:\n",
    "    def count_properties_sold(row: pd.DataFrame) -> int:\n",
    "        if row.date and row.postgroup and row.property_type:\n",
    "            slice = ix.loc[row.date - pd.Timedelta(days=n_days) : row.date, row.postgroup, row.property_type]\n",
    "            return slice.sum()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return count_properties_sold\n",
    "\n",
    "\n",
    "# https://www.gov.uk/guidance/about-the-price-paid-data\n",
    "ppd_property_type = {\"D\": \"detached\", \"S\": \"semi-detached\", \"T\": \"terraced\", \"F\": \"flat/maisonettes\"}\n",
    "ppd_duration = {\"F\": \"freehold\", \"L\": \"leasehold\"}\n",
    "ppd_old_or_new = {\"Y\": \"new\", \"N\": \"old\"}\n",
    "ppd_cols_src = [\n",
    "    \"id\",\n",
    "    \"price\",\n",
    "    \"date\",\n",
    "    \"postcode\",\n",
    "    \"property_type\",\n",
    "    \"old_or_new\",\n",
    "    \"duration\",\n",
    "    \"paon\",\n",
    "    \"saon\",\n",
    "    \"street\",\n",
    "    \"locality\",\n",
    "    \"town_city\",\n",
    "    \"district\",\n",
    "    \"county\",\n",
    "    \"ppd_category_type\",\n",
    "    \"record_status\",\n",
    "]\n",
    "ppdf_cols_dst = [\n",
    "    \"date\",\n",
    "    \"postgroup\",\n",
    "    \"property_type\",\n",
    "    \"old_or_new\",\n",
    "    \"duration\",\n",
    "    \"price\",\n",
    "]\n",
    "ppd_filepaths = list(ppd_folderpath.glob(\"*.zip\"))\n",
    "ppd_df = pd.concat([pd.read_csv(fp, names=ppd_cols_src) for fp in tqdm(ppd_filepaths)])\n",
    "ppd_df[\"postgroup\"] = ppd_df[\"postcode\"].progress_map(lambda x: str(x).split(\" \")[0])\n",
    "ppd_df[\"date\"] = pd.to_datetime(ppd_df[\"date\"])\n",
    "ppd_df[\"property_type\"] = ppd_df[\"property_type\"].progress_map(ppd_property_type.get)\n",
    "ppd_df[\"duration\"] = ppd_df[\"duration\"].progress_map(ppd_duration.get)\n",
    "ppd_df[\"old_or_new\"] = ppd_df[\"old_or_new\"].progress_map(ppd_old_or_new.get)\n",
    "ppd_df[\"price\"] = ppd_df[\"price\"].astype(\"float\")\n",
    "ppd_df = ppd_df[ppdf_cols_dst]\n",
    "ppd_df = ppd_df.dropna()\n",
    "ppd_df = ppd_df.astype({c: \"category\" for c in [\"postgroup\", \"property_type\", \"old_or_new\", \"duration\"]})\n",
    "ppd_df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from datetime import date\n",
    "\n",
    "inflation_date_pattern = re.compile(r\"([\\d]{4})(?:\\s+([\\w]{3}))?\")\n",
    "inflation_month_names = [\n",
    "    \"JAN\",\n",
    "    \"FEB\",\n",
    "    \"MAR\",\n",
    "    \"APR\",\n",
    "    \"MAY\",\n",
    "    \"JUN\",\n",
    "    \"JUL\",\n",
    "    \"AUG\",\n",
    "    \"SEP\",\n",
    "    \"OCT\",\n",
    "    \"NOV\",\n",
    "    \"DEC\",\n",
    "]\n",
    "inflation_month_index = {mn: ix + 1 for (ix, mn) in enumerate(inflation_month_names)}\n",
    "inflation_month_index[\"Q1\"] = 1\n",
    "inflation_month_index[\"Q2\"] = 4\n",
    "inflation_month_index[\"Q3\"] = 7\n",
    "inflation_month_index[\"Q3\"] = 10\n",
    "\n",
    "inflation_acceptable_numeric_chars = string.digits + \".,\"\n",
    "\n",
    "\n",
    "def extract_inflation_date(x: str) -> date:\n",
    "    match = next(inflation_date_pattern.finditer(x), None)\n",
    "    if match:\n",
    "        group_count = len(match.groups())\n",
    "        if group_count >= 1:\n",
    "            year = int(match.group(1))\n",
    "            month = 1\n",
    "            month_name = match.group(2)\n",
    "            if group_count > 1 and month_name:\n",
    "                month_name = month_name.strip().upper()\n",
    "                month = inflation_month_index.get(month_name)\n",
    "            return date(year, month, 1)\n",
    "\n",
    "\n",
    "def extract_inflation_rate(x: str) -> float:\n",
    "    x = str(x)\n",
    "    if all([c in inflation_acceptable_numeric_chars for c in x]):\n",
    "        return float(x)\n",
    "    return None\n",
    "\n",
    "\n",
    "inflation_df = pd.read_csv(inflation_filepath)\n",
    "inflation_df[\"date\"] = inflation_df[\"Title\"].map(extract_inflation_date)\n",
    "inflation_df[\"date\"] = pd.to_datetime(inflation_df[\"date\"])\n",
    "inflation_df[\"rate\"] = inflation_df[\"CPIH ANNUAL RATE 00: ALL ITEMS 2015=100\"].map(extract_inflation_rate)\n",
    "inflation_df[\"rate\"] = inflation_df[\"rate\"].astype(\"float\", errors=\"ignore\")\n",
    "inflation_df = inflation_df[[\"date\", \"rate\"]]\n",
    "inflation_df = inflation_df.dropna()\n",
    "inflation_df = inflation_df.set_index(\"date\").sort_index()\n",
    "inflation_df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_df = pd.read_csv(interest_filepath)\n",
    "interest_df[\"date\"] = pd.to_datetime(interest_df[\"Date Changed\"])\n",
    "interest_df[\"rate\"] = interest_df[\"Rate\"].astype(\"float\")\n",
    "interest_df = interest_df[[\"date\", \"rate\"]]\n",
    "interest_df = interest_df.set_index(\"date\").sort_index()\n",
    "interest_df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from typing import Callable\n",
    "from datetime import date, timedelta\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def build_rate_extractor(df: pd.DataFrame) -> Callable[[date], float]:\n",
    "    min_date = df.index.min()\n",
    "    max_date = df.index.max()\n",
    "    cur_date = min_date\n",
    "    rate_index = {}\n",
    "    first_rate = df.rate[0]\n",
    "    prev_rate = first_rate\n",
    "    last_rate = df.rate[-1]\n",
    "    with trange((max_date - min_date).days, desc=\"rate_index\") as pbar:\n",
    "        while cur_date <= max_date:\n",
    "            rates = df[df.index == cur_date].rate\n",
    "            if rates.any():\n",
    "                new_rate = rates[0] / 100.0\n",
    "                rate_index[cur_date] = new_rate\n",
    "                prev_rate = new_rate\n",
    "            else:\n",
    "                rate_index[cur_date] = prev_rate\n",
    "            cur_date += timedelta(days=1)\n",
    "            pbar.update()\n",
    "\n",
    "    def get_rate_for_date(d: date) -> float:\n",
    "        if d < min_date:\n",
    "            return first_rate\n",
    "        elif d > max_date:\n",
    "            return last_rate\n",
    "        else:\n",
    "            return rate_index[d]\n",
    "\n",
    "    return get_rate_for_date\n",
    "\n",
    "\n",
    "df = ppd_df.copy()\n",
    "df[\"inflation_rate\"] = df.date.progress_map(build_rate_extractor(df=inflation_df))\n",
    "df[\"interest_rate\"] = df.date.progress_map(build_rate_extractor(df=interest_df))\n",
    "df[\"date_year\"] = df.date.progress_map(lambda d: d.year)\n",
    "df[\"date_month\"] = df.date.progress_map(lambda d: d.month)\n",
    "df[\"date_day\"] = df.date.progress_map(lambda d: d.day)\n",
    "df[\"date_day_of_week\"] = df.date.progress_map(lambda d: d.weekday())\n",
    "df = df.sort_values(by=\"date\").reset_index()\n",
    "df = df[\n",
    "    [\"date_year\", \"date_month\", \"date_day\", \"date_day_of_week\"]\n",
    "    + list(ppd_df.columns[1:-1])\n",
    "    + [\"inflation_rate\", \"interest_rate\", \"price\"]\n",
    "]\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(data_folderpath / \"snapshot-Xy-1NF.zip\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert df is not None\n",
    "except NameError:\n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"[SNAPSHOT] Reloading...\")\n",
    "    pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n",
    "    data_folderpath = pathlib.Path(\"./data\")\n",
    "    df = pd.read_csv(data_folderpath / \"snapshot-Xy-1NF.zip\").astype(\n",
    "        {\"postgroup\": \"category\", \"property_type\": \"category\", \"old_or_new\": \"category\", \"duration\": \"category\"}\n",
    "    )\n",
    "    print(f\" - reloaded from snapshot, {df.shape[0]}\")\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats_continuous = df.select_dtypes(include=\"float\")\n",
    "df_feats_continuous.sample(n=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Tendency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central_tendency = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(df_feats_continuous.mean(), columns=[\"mean\"]).transpose(),\n",
    "        pd.DataFrame(df_feats_continuous.median(), columns=[\"median\"]).transpose(),\n",
    "        pd.DataFrame([df_feats_continuous[c].mode()[0] for c in df_feats_continuous.columns], columns=[\"mode\"])\n",
    "        .set_index(df_feats_continuous.columns)\n",
    "        .transpose(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_central_tendency\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_measures_of_spread = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(df_feats_continuous.var(), columns=[\"var\"]).transpose(),\n",
    "        df_feats_continuous.describe(),\n",
    "        pd.DataFrame(\n",
    "            df_feats_continuous.quantile(0.75) - df_feats_continuous.quantile(0.25), columns=[\"IQR\"]\n",
    "        ).transpose(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "df_measures_of_spread\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types_of_distros = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(df_feats_continuous.skew(), columns=[\"skew\"]).transpose(),\n",
    "        pd.DataFrame(df_feats_continuous.kurtosis(), columns=[\"kurtosis\"]).transpose(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_types_of_distros\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert df is not None\n",
    "except NameError:\n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"[SNAPSHOT] Reloading...\")\n",
    "    pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n",
    "    data_folderpath = pathlib.Path(\"./data\")\n",
    "    df = pd.read_csv(data_folderpath / \"snapshot-Xy-1NF.zip\").astype(\n",
    "        {\n",
    "            \"postgroup\": \"category\",\n",
    "            \"property_type\": \"category\",\n",
    "            \"old_or_new\": \"category\",\n",
    "            \"duration\": \"category\",\n",
    "            \"price\": \"double\",\n",
    "        }\n",
    "    )\n",
    "    print(f\" - reloaded from snapshot, {df.shape[0]}\")\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats_continuous = df.select_dtypes(include=\"float\")\n",
    "df_feats_continuous.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Continuous Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats_cont_scal_pt = df[list(df_feats_continuous.columns) + [\"property_type\"]]\n",
    "df_feats_cont_scal_pt = df_feats_cont_scal_pt.sample(n=int(df_feats_cont_scal_pt.shape[0] * 0.01))\n",
    "df_feats_cont_scal_pt = df_feats_cont_scal_pt.copy()\n",
    "df_feats_cont_scal_pt[\"inflation_rate\"] *= 100.0\n",
    "df_feats_cont_scal_pt[\"interest_rate\"] *= 100.0\n",
    "df_feats_cont_scal_pt.rename(\n",
    "    columns={\n",
    "        \"inflation_rate\": \"inflation_rate (%)\",\n",
    "        \"interest_rate\": \"interest_rate (%)\",\n",
    "        \"price\": \"price (millions of £)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "sns.pairplot(\n",
    "    data=df_feats_cont_scal_pt,\n",
    "    hue=\"property_type\",\n",
    "    kind=\"scatter\",\n",
    "    diag_kind=\"kde\",\n",
    "    palette=\"viridis\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions for Inflation Rate & Interest Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rate_distributions(ax, df: pd.DataFrame, label: str, color: str):\n",
    "    df = df.copy()\n",
    "    df[\"rate\"] = df[\"rate\"] * 100.0\n",
    "    x = np.linspace(0.0, df[\"rate\"].max(), 100)\n",
    "    df[\"bin\"] = pd.cut(df[\"rate\"], bins=x)\n",
    "    y = list(df.groupby(\"bin\").count()[\"rate\"])\n",
    "    ax.fill_between(x[:-1], 0.0, y, color=color, alpha=0.5)\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter(\"%2.2f%%\"))\n",
    "    intervals = [0.05, 0.5, 0.95]\n",
    "    for interval, quantile in zip(intervals, df.rate.quantile(intervals)):\n",
    "        percentile = f\"P{int(interval*100.)}={round(quantile, 2)}\"\n",
    "        bbox = dict(boxstyle=\"round, pad=0.3\", fc=\"lightgray\", lw=2)\n",
    "        ax.axvline(x=quantile, color=\"blue\")\n",
    "        ax.annotate(\n",
    "            percentile,\n",
    "            xy=(quantile, max(y)),\n",
    "            bbox=bbox,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "    ax.axvline(x=quantile, color=\"blue\")\n",
    "    ax.legend([label], loc=\"lower center\", bbox_to_anchor=(0.5, -0.2))\n",
    "\n",
    "\n",
    "df_daily_means = df.groupby([\"date_year\", \"date_month\", \"date_day\"]).mean(numeric_only=True)\n",
    "df_daily_means_interest = df_daily_means[[\"interest_rate\"]].rename(columns={\"interest_rate\": \"rate\"})\n",
    "df_daily_means_inflation = df_daily_means[[\"inflation_rate\"]].rename(columns={\"inflation_rate\": \"rate\"})\n",
    "_, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 3))\n",
    "plot_rate_distributions(\n",
    "    ax=axes[0],\n",
    "    df=df_daily_means_interest,\n",
    "    label=\"interest\",\n",
    "    color=\"green\",\n",
    ")\n",
    "plot_rate_distributions(\n",
    "    ax=axes[1],\n",
    "    df=df_daily_means_inflation,\n",
    "    label=\"inflation\",\n",
    "    color=\"red\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Distribution Inflation, Interest & Property Price per Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "def plot_inflation_and_interest(\n",
    "    ax: plt.Axes,\n",
    "    df_mean_by_date: pd.DataFrame,\n",
    "    xlim_left: float,\n",
    "    xlim_right: float,\n",
    "):\n",
    "    df_mean_by_date = df_mean_by_date.reset_index()\n",
    "    x = df_mean_by_date.apply(lambda r: date(int(r.date_year), int(r.date_month), int(r.date_day)), axis=1)\n",
    "    ax.grid(visible=True)\n",
    "    ax.plot(x, df_mean_by_date.interest_rate * 100.0, \"g.-\", alpha=0.7)\n",
    "    ax.plot(x, df_mean_by_date.inflation_rate * 100.0, \"r.-\", alpha=0.7)\n",
    "    ax.set_xlim(left=xlim_left, right=xlim_right)\n",
    "    ax.set_ylabel(\"rates (%)\")\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%2.2f%%\"))\n",
    "    ax.legend([\"interest\", \"inflation\"])\n",
    "\n",
    "\n",
    "def plot_prices_per_property_type(\n",
    "    ax: plt.Axes,\n",
    "    df_mean_by_date_and_pt: pd.DataFrame,\n",
    "):\n",
    "    df_mean_by_date_and_pt = df_mean_by_date_and_pt.reset_index()\n",
    "    ax.grid(visible=True)\n",
    "    ax.yaxis.set_major_formatter(StrMethodFormatter(\"{x:,}\"))\n",
    "    ax.set_ylim(0.0, df_mean_by_date_and_pt.price.quantile(0.95) * 1.2)\n",
    "    ax.set_ylabel(\"property price (£)\")\n",
    "    property_types = sorted(df_mean_by_date_and_pt.property_type.unique())\n",
    "    for ix, property_type in tqdm(list(enumerate(property_types))):\n",
    "        sub_series = df_mean_by_date_and_pt[df_mean_by_date_and_pt.property_type == property_type].copy()\n",
    "        sub_series = sub_series.reset_index().groupby([\"date_year\", \"date_month\"]).mean(numeric_only=True).reset_index()\n",
    "        sub_series = sub_series.fillna(method=\"ffill\")\n",
    "        x = sub_series.apply(lambda r: date(int(r.date_year), int(r.date_month), 1), axis=1)\n",
    "        ax.plot(x, sub_series.price, \"s\", alpha=0.7)\n",
    "        ax.legend(property_types)\n",
    "\n",
    "\n",
    "min_intersecting_date = date(df.date_year.min(), 1, 1)\n",
    "max_intersecting_date = date(df.date_year.max(), 12, 30)\n",
    "_, axes = plt.subplots(nrows=2, figsize=(10, 8), sharex=True)\n",
    "plot_inflation_and_interest(\n",
    "    ax=axes[0],\n",
    "    df_mean_by_date=df.groupby([\"date_year\", \"date_month\", \"date_day\"]).mean(numeric_only=True),\n",
    "    xlim_left=min_intersecting_date,\n",
    "    xlim_right=max_intersecting_date,\n",
    ")\n",
    "plot_prices_per_property_type(\n",
    "    ax=axes[1],\n",
    "    df_mean_by_date_and_pt=df.groupby([\"date_year\", \"date_month\", \"date_day\", \"property_type\"]).mean(numeric_only=True),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Reloading...\n",
      " - reloaded from snapshot, 4336841\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_year</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_day</th>\n",
       "      <th>date_day_of_week</th>\n",
       "      <th>postgroup</th>\n",
       "      <th>property_type</th>\n",
       "      <th>old_or_new</th>\n",
       "      <th>duration</th>\n",
       "      <th>inflation_rate</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2528347</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>EX37</td>\n",
       "      <td>detached</td>\n",
       "      <td>old</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>950,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949714</th>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>GL2</td>\n",
       "      <td>flat/maisonettes</td>\n",
       "      <td>new</td>\n",
       "      <td>leasehold</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>105,750.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203498</th>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>NR7</td>\n",
       "      <td>flat/maisonettes</td>\n",
       "      <td>old</td>\n",
       "      <td>leasehold</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>166,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390271</th>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>SY8</td>\n",
       "      <td>semi-detached</td>\n",
       "      <td>old</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.005</td>\n",
       "      <td>247,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597671</th>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>ST10</td>\n",
       "      <td>semi-detached</td>\n",
       "      <td>old</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.007</td>\n",
       "      <td>90,000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_year  date_month  date_day  date_day_of_week postgroup  \\\n",
       "2528347       2020          10        30                 4      EX37   \n",
       "2949714       2021           3        12                 4       GL2   \n",
       "3203498       2021           5        28                 4       NR7   \n",
       "390271        2018           6        15                 4       SY8   \n",
       "597671        2018           8        24                 4      ST10   \n",
       "\n",
       "            property_type old_or_new   duration  inflation_rate  \\\n",
       "2528347          detached        old   freehold           0.009   \n",
       "2949714  flat/maisonettes        new  leasehold           0.010   \n",
       "3203498  flat/maisonettes        old  leasehold           0.021   \n",
       "390271      semi-detached        old   freehold           0.023   \n",
       "597671      semi-detached        old   freehold           0.024   \n",
       "\n",
       "         interest_rate       price  \n",
       "2528347          0.001 950,000.000  \n",
       "2949714          0.001 105,750.000  \n",
       "3203498          0.001 166,000.000  \n",
       "390271           0.005 247,000.000  \n",
       "597671           0.007  90,000.000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    assert df is not None\n",
    "except NameError:\n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"[SNAPSHOT] Reloading...\")\n",
    "    pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n",
    "    data_folderpath = pathlib.Path(\"./data\")\n",
    "    df = pd.read_csv(data_folderpath / \"snapshot-Xy-1NF.zip\").astype(\n",
    "        {\n",
    "            \"postgroup\": \"category\",\n",
    "            \"property_type\": \"category\",\n",
    "            \"old_or_new\": \"category\",\n",
    "            \"duration\": \"category\",\n",
    "            \"price\": \"double\",\n",
    "        }\n",
    "    )\n",
    "    print(f\" - reloaded from snapshot, {df.shape[0]}\")\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('results/20221228_1112')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "results_dir = pathlib.Path(f\"./results/{datetime.now().strftime('%Y%m%d_%H%M')}\")\n",
    "shutil.rmtree(results_dir, ignore_errors=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matrix & Targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[df.columns[:-1]], df[df.columns[-1]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out Test-Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>|X|</th>\n",
       "      <th>|y|</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full</td>\n",
       "      <td>4336841</td>\n",
       "      <td>4336841</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>4293472</td>\n",
       "      <td>4293472</td>\n",
       "      <td>99.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>43369</td>\n",
       "      <td>43369</td>\n",
       "      <td>1.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split      |X|      |y|       %\n",
       "0   full  4336841  4336841  100.0%\n",
       "1  train  4293472  4293472   99.0%\n",
       "2   test    43369    43369    1.0%"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def produce_split_summary(\n",
    "    X_split: pd.DataFrame, y_split: pd.DataFrame, name: str, total: int\n",
    ") -> Tuple[str, int, int, str]:\n",
    "    return (\n",
    "        name,\n",
    "        X_split.shape[0],\n",
    "        y_split.shape[0],\n",
    "        \"{:.1f}%\".format(100.0 * X_split.shape[0] / total),\n",
    "    )\n",
    "\n",
    "\n",
    "r = 42\n",
    "train_size = 0.99\n",
    "X1, X2, y1, y2 = train_test_split(X, y, train_size=train_size, random_state=r)\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        produce_split_summary(X, y, \"full\", total=X.shape[0]),\n",
    "        produce_split_summary(X1, y1, \"train\", total=X.shape[0]),\n",
    "        produce_split_summary(X2, y2, \"test\", total=X.shape[0]),\n",
    "    ],\n",
    "    columns=[\"split\", \"|X|\", \"|y|\", \"%\"],\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[svd_polynomial_regression]\n",
      "[Pipeline] . (step 1 of 4) Processing columntransformer, total=   8.7s\n",
      "[Pipeline] ...... (step 2 of 4) Processing truncatedsvd, total=   4.4s\n",
      "[Pipeline]  (step 3 of 4) Processing polynomialfeatures, total=   0.0s\n",
      "[Pipeline] .. (step 4 of 4) Processing linearregression, total=   0.2s\n",
      "- train: mae=173341.90528655183\n",
      "- test : mae=174260.0219136929\n",
      "[mlp_regression]\n",
      "[Pipeline] . (step 1 of 2) Processing columntransformer, total=  11.5s\n",
      "Iteration 1, loss = 131560605211.27497864\n",
      "Validation score: -0.741920\n",
      "[Pipeline] ...... (step 2 of 2) Processing mlpregressor, total=  18.2s\n",
      "- train: mae=318083.0113977656\n",
      "- test : mae=318492.4474973322\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def make_sine_cycle_encoder(period: int = 1) -> float:\n",
    "    assert period != 0\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use\n",
    "def make_df_column_transformer(scale_non_categorical: bool = False):\n",
    "    categorical_selector = make_column_selector(dtype_include=\"category\")\n",
    "    non_categorical_selector = make_column_selector(dtype_exclude=\"category\")\n",
    "    one_hot = OneHotEncoder(sparse_output=True, handle_unknown=\"ignore\")\n",
    "    cycle_sine_12 = make_sine_cycle_encoder(period=12)\n",
    "    cycle_sine_31 = make_sine_cycle_encoder(period=31)\n",
    "    cycle_sine_6 = make_sine_cycle_encoder(period=6)\n",
    "    steps = [\n",
    "        (one_hot, categorical_selector),\n",
    "        (cycle_sine_12, [\"date_month\"]),\n",
    "        (cycle_sine_31, [\"date_day\"]),\n",
    "        (cycle_sine_6, [\"date_day_of_week\"]),\n",
    "    ]\n",
    "    if scale_non_categorical:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        steps.append((scaler, non_categorical_selector))\n",
    "    return make_column_transformer(*steps, remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "def make_svd_polynomial_regression_model(\n",
    "    svd_dims: int = 1,\n",
    "    polynomial_degree: int = 1,\n",
    "    fit_intercept: bool = False,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    return make_pipeline(\n",
    "        make_df_column_transformer(),\n",
    "        TruncatedSVD(n_components=svd_dims, algorithm=\"arpack\", random_state=r),\n",
    "        PolynomialFeatures(degree=polynomial_degree, include_bias=False),\n",
    "        LinearRegression(fit_intercept=fit_intercept),\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def make_mlp_regression_model(\n",
    "    embedding_layer_dims: int = None,\n",
    "    residual_layer_dims: int = 1,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    hidden_layer_sizes = []\n",
    "    if embedding_layer_dims:\n",
    "        hidden_layer_sizes.append(embedding_layer_dims)\n",
    "    if residual_layer_dims:\n",
    "        hidden_layer_sizes.append(residual_layer_dims)\n",
    "    return make_pipeline(\n",
    "        make_df_column_transformer(scale_non_categorical=True),\n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=tuple(hidden_layer_sizes),\n",
    "            solver=\"adam\",\n",
    "            alpha=0.0001,\n",
    "            max_iter=1,\n",
    "            early_stopping=True,\n",
    "            random_state=random_state,\n",
    "            verbose=True,\n",
    "        ),\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "r = 42\n",
    "models = {\n",
    "    \"svd_polynomial_regression\": make_svd_polynomial_regression_model(random_state=r),\n",
    "    \"mlp_regression\": make_mlp_regression_model(random_state=r),\n",
    "}\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"[{model_name}]\")\n",
    "    model_instance.fit(X1, y1)\n",
    "    print(f\"- train: mae={mean_absolute_error(model_instance.predict(X1), y1)}\")\n",
    "    print(f\"- test : mae={mean_absolute_error(model_instance.predict(X2), y2)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def make_grid(\n",
    "    model: Pipeline,\n",
    "    param_grid: Dict[str, Any],\n",
    ") -> GridSearchCV:\n",
    "    return GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "\n",
    "def export_grid_results(\n",
    "    grid: GridSearchCV,\n",
    "    results_dir: pathlib.Path,\n",
    "    experiment_name: str,\n",
    "):\n",
    "    experiment_dir = results_dir / experiment_name\n",
    "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_best_params = pd.DataFrame.from_dict(grid.best_params_, orient=\"index\")\n",
    "    df_best_params.to_csv(experiment_dir / \"best_params.csv\", header=False)\n",
    "    print(df_best_params)\n",
    "\n",
    "    df_metrics = pd.DataFrame.from_dict(grid.cv_results_)\n",
    "    df_metrics = df_metrics.sort_values(by=\"rank_test_score\").reset_index(drop=True)\n",
    "    df_metrics_cols_key = [\"rank_test_score\"]\n",
    "    df_metrics_cols_key.extend([c for c in df_metrics.columns.tolist() if c.startswith(\"param_\")])\n",
    "    df_metrics_cols_key.extend([\"mean_test_score\", \"std_test_score\"])\n",
    "    df_results_cols_rest = [c for c in df_metrics.columns.tolist() if c not in df_metrics_cols_key]\n",
    "    df_results_cols_rest.remove(\"params\")\n",
    "    df_metrics = df_metrics[df_metrics_cols_key + df_results_cols_rest]\n",
    "    df_metrics.to_csv(experiment_dir / \"grid_metrics.csv\", index=False)\n",
    "    return df_metrics\n",
    "\n",
    "\n",
    "def run_grid_experiment(\n",
    "    experiment: str,\n",
    "    model: Pipeline,\n",
    "    training_data: Tuple[np.array, np.array],\n",
    "    test_data: Tuple[np.array, np.array],\n",
    "    param_grid: Dict[str, Any],\n",
    "    results_dir: pathlib.Path,\n",
    "):\n",
    "    print(f\"[{experiment}]\")\n",
    "    grid = make_grid(model, param_grid)\n",
    "    X_train, y_train = training_data\n",
    "    grid.fit(X_train, y_train)\n",
    "    df_metrics = export_grid_results(grid, results_dir, experiment)\n",
    "    X2, y2 = test_data\n",
    "    mean_absolute_error(model.predict(X2), y2)\n",
    "    return df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Polynomial Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[svd_polynomial_regression]\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[Pipeline] . (step 1 of 4) Processing columntransformer, total=   6.5s\n",
      "[Pipeline] ...... (step 2 of 4) Processing truncatedsvd, total=   3.9s\n",
      "[Pipeline]  (step 3 of 4) Processing polynomialfeatures, total=   0.2s\n",
      "[Pipeline] .. (step 4 of 4) Processing linearregression, total=   1.0s\n",
      "[CV 1/5] END linearregression__fit_intercept=True, polynomialfeatures__degree=2, truncatedsvd__n_components=2;, score=-172665.868 total time=  13.8s\n",
      "[Pipeline] . (step 1 of 4) Processing columntransformer, total=   7.0s\n",
      "[Pipeline] ...... (step 2 of 4) Processing truncatedsvd, total=   3.3s\n",
      "[Pipeline]  (step 3 of 4) Processing polynomialfeatures, total=   0.2s\n",
      "[Pipeline] .. (step 4 of 4) Processing linearregression, total=   1.2s\n",
      "[CV 2/5] END linearregression__fit_intercept=True, polynomialfeatures__degree=2, truncatedsvd__n_components=2;, score=-172366.594 total time=  13.8s\n",
      "[Pipeline] . (step 1 of 4) Processing columntransformer, total=   8.3s\n",
      "[Pipeline] ...... (step 2 of 4) Processing truncatedsvd, total=   3.7s\n",
      "[Pipeline]  (step 3 of 4) Processing polynomialfeatures, total=   0.2s\n",
      "[Pipeline] .. (step 4 of 4) Processing linearregression, total=   1.9s\n",
      "[CV 3/5] END linearregression__fit_intercept=True, polynomialfeatures__degree=2, truncatedsvd__n_components=2;, score=-172351.599 total time=  16.7s\n"
     ]
    }
   ],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"svd_polynomial_regression\",\n",
    "    model=models[\"svd_polynomial_regression\"],\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"truncatedsvd__n_components\": [2 ** (p + 1) for p in range(0, 3)],\n",
    "        \"polynomialfeatures__degree\": [2 ** (p + 1) for p in range(0, 2)],\n",
    "        \"linearregression__fit_intercept\": [True, False],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: MLP Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hidden_layer_sizes = []\n",
    "# embedding layer only\n",
    "mlp_hidden_layer_sizes.extend([(2,), (4,), (8,)])\n",
    "# small embeddings + residual\n",
    "mlp_hidden_layer_sizes.extend([(2, 2), (2, 4)])\n",
    "# medium embeddings (attempted repr: time, interest & inflation) + residual\n",
    "mlp_hidden_layer_sizes.extend([(4, 4), (4, 8)])\n",
    "# large embeddings (attempted repr: time, time-relationships, rates & relationships) + residual\n",
    "mlp_hidden_layer_sizes.extend([(8, 8), (8, 16)])\n",
    "\n",
    "run_grid_experiment(\n",
    "    experiment=f\"mlp_regression\",\n",
    "    model=models[\"mlp_regression\"],\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"mlpregressor__hidden_layer_sizes\": mlp_hidden_layer_sizes,\n",
    "        \"mlpregressor__solver\": [\"adam\", \"lbfgs\"],\n",
    "        \"mlpregressor__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "        \"mlpregressor__max_iter\": [25],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Conclusions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing remarks/statements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff2a5bf76a5a6a6e16a446abb6a1160221241a1606404c7f1c7e55bf8cf0847f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
