{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Assignment: Building a Regression Model\n",
    "\n",
    "```\n",
    "University of London\n",
    "BSc in Computer Science\n",
    "CM3005, Data Science\n",
    "Hudson Leonardo MENDES\n",
    "hlm12@student.london.ac.uk\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-specific area\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "data_folderpath = pathlib.Path(\"./data\")\n",
    "\n",
    "ppd_folderpath = data_folderpath / \"uk-ppd\"\n",
    "inflation_filepath = data_folderpath / \"uk-ons/ons-inflation-1989-2022.csv\"\n",
    "interest_filepath = data_folderpath / \"uk-boe/boe-interest-1975-2022.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def build_count_properties_sold(ix: pd.DataFrame, n_days: int) -> Callable[[pd.DataFrame], int]:\n",
    "    def count_properties_sold(row: pd.DataFrame) -> int:\n",
    "        if row.date and row.postgroup and row.property_type:\n",
    "            slice = ix.loc[row.date - pd.Timedelta(days=n_days) : row.date, row.postgroup, row.property_type]\n",
    "            return slice.sum()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return count_properties_sold\n",
    "\n",
    "\n",
    "# https://www.gov.uk/guidance/about-the-price-paid-data\n",
    "ppd_property_type = {\"D\": \"detached\", \"S\": \"semi-detached\", \"T\": \"terraced\", \"F\": \"flat/maisonettes\"}\n",
    "ppd_duration = {\"F\": \"freehold\", \"L\": \"leasehold\"}\n",
    "ppd_old_or_new = {\"Y\": \"new\", \"N\": \"old\"}\n",
    "ppd_cols_src = [\n",
    "    \"id\",\n",
    "    \"price\",\n",
    "    \"date\",\n",
    "    \"postcode\",\n",
    "    \"property_type\",\n",
    "    \"old_or_new\",\n",
    "    \"duration\",\n",
    "    \"paon\",\n",
    "    \"saon\",\n",
    "    \"street\",\n",
    "    \"locality\",\n",
    "    \"town_city\",\n",
    "    \"district\",\n",
    "    \"county\",\n",
    "    \"ppd_category_type\",\n",
    "    \"record_status\",\n",
    "]\n",
    "ppdf_cols_dst = [\n",
    "    \"date\",\n",
    "    \"postgroup\",\n",
    "    \"property_type\",\n",
    "    \"old_or_new\",\n",
    "    \"duration\",\n",
    "    \"price\",\n",
    "]\n",
    "ppd_filepaths = list(ppd_folderpath.glob(\"*.zip\"))\n",
    "ppd_df = pd.concat([pd.read_csv(fp, names=ppd_cols_src) for fp in tqdm(ppd_filepaths)])\n",
    "ppd_df[\"postgroup\"] = ppd_df[\"postcode\"].progress_map(lambda x: str(x).split(\" \")[0])\n",
    "ppd_df[\"date\"] = pd.to_datetime(ppd_df[\"date\"])\n",
    "ppd_df[\"property_type\"] = ppd_df[\"property_type\"].progress_map(ppd_property_type.get)\n",
    "ppd_df[\"duration\"] = ppd_df[\"duration\"].progress_map(ppd_duration.get)\n",
    "ppd_df[\"old_or_new\"] = ppd_df[\"old_or_new\"].progress_map(ppd_old_or_new.get)\n",
    "ppd_df[\"price\"] = ppd_df[\"price\"].astype(\"float\")\n",
    "ppd_df = ppd_df[ppdf_cols_dst]\n",
    "ppd_df = ppd_df.dropna()\n",
    "ppd_df = ppd_df.astype({c: \"category\" for c in [\"postgroup\", \"property_type\", \"old_or_new\", \"duration\"]})\n",
    "ppd_df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from datetime import date\n",
    "\n",
    "inflation_date_pattern = re.compile(r\"([\\d]{4})(?:\\s+([\\w]{3}))?\")\n",
    "inflation_month_names = [\n",
    "    \"JAN\",\n",
    "    \"FEB\",\n",
    "    \"MAR\",\n",
    "    \"APR\",\n",
    "    \"MAY\",\n",
    "    \"JUN\",\n",
    "    \"JUL\",\n",
    "    \"AUG\",\n",
    "    \"SEP\",\n",
    "    \"OCT\",\n",
    "    \"NOV\",\n",
    "    \"DEC\",\n",
    "]\n",
    "inflation_month_index = {mn: ix + 1 for (ix, mn) in enumerate(inflation_month_names)}\n",
    "inflation_month_index[\"Q1\"] = 1\n",
    "inflation_month_index[\"Q2\"] = 4\n",
    "inflation_month_index[\"Q3\"] = 7\n",
    "inflation_month_index[\"Q3\"] = 10\n",
    "\n",
    "inflation_acceptable_numeric_chars = string.digits + \".,\"\n",
    "\n",
    "\n",
    "def extract_inflation_date(x: str) -> date:\n",
    "    match = next(inflation_date_pattern.finditer(x), None)\n",
    "    if match:\n",
    "        group_count = len(match.groups())\n",
    "        if group_count >= 1:\n",
    "            year = int(match.group(1))\n",
    "            month = 1\n",
    "            month_name = match.group(2)\n",
    "            if group_count > 1 and month_name:\n",
    "                month_name = month_name.strip().upper()\n",
    "                month = inflation_month_index.get(month_name)\n",
    "            return date(year, month, 1)\n",
    "\n",
    "\n",
    "def extract_inflation_rate(x: str) -> float:\n",
    "    x = str(x)\n",
    "    if all([c in inflation_acceptable_numeric_chars for c in x]):\n",
    "        return float(x)\n",
    "    return None\n",
    "\n",
    "\n",
    "inflation_df = pd.read_csv(inflation_filepath)\n",
    "inflation_df[\"date\"] = inflation_df[\"Title\"].map(extract_inflation_date)\n",
    "inflation_df[\"date\"] = pd.to_datetime(inflation_df[\"date\"])\n",
    "inflation_df[\"rate\"] = inflation_df[\"CPIH ANNUAL RATE 00: ALL ITEMS 2015=100\"].map(extract_inflation_rate)\n",
    "inflation_df[\"rate\"] = inflation_df[\"rate\"].astype(\"float\", errors=\"ignore\")\n",
    "inflation_df = inflation_df[[\"date\", \"rate\"]]\n",
    "inflation_df = inflation_df.dropna()\n",
    "inflation_df = inflation_df.set_index(\"date\").sort_index()\n",
    "inflation_df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_df = pd.read_csv(interest_filepath)\n",
    "interest_df[\"date\"] = pd.to_datetime(interest_df[\"Date Changed\"])\n",
    "interest_df[\"rate\"] = interest_df[\"Rate\"].astype(\"float\")\n",
    "interest_df = interest_df[[\"date\", \"rate\"]]\n",
    "interest_df = interest_df.set_index(\"date\").sort_index()\n",
    "interest_df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from typing import Callable\n",
    "from datetime import date, timedelta\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def build_rate_extractor(df: pd.DataFrame) -> Callable[[date], float]:\n",
    "    min_date = df.index.min()\n",
    "    max_date = df.index.max()\n",
    "    cur_date = min_date\n",
    "    rate_index = {}\n",
    "    first_rate = df.rate[0]\n",
    "    prev_rate = first_rate\n",
    "    last_rate = df.rate[-1]\n",
    "    with trange((max_date - min_date).days, desc=\"rate_index\") as pbar:\n",
    "        while cur_date <= max_date:\n",
    "            rates = df[df.index == cur_date].rate\n",
    "            if rates.any():\n",
    "                new_rate = rates[0] / 100.0\n",
    "                rate_index[cur_date] = new_rate\n",
    "                prev_rate = new_rate\n",
    "            else:\n",
    "                rate_index[cur_date] = prev_rate\n",
    "            cur_date += timedelta(days=1)\n",
    "            pbar.update()\n",
    "\n",
    "    def get_rate_for_date(d: date) -> float:\n",
    "        if d < min_date:\n",
    "            return first_rate\n",
    "        elif d > max_date:\n",
    "            return last_rate\n",
    "        else:\n",
    "            return rate_index[d]\n",
    "\n",
    "    return get_rate_for_date\n",
    "\n",
    "\n",
    "df = ppd_df.copy()\n",
    "df[\"inflation_rate\"] = df.date.progress_map(build_rate_extractor(df=inflation_df))\n",
    "df[\"interest_rate\"] = df.date.progress_map(build_rate_extractor(df=interest_df))\n",
    "df[\"date_year\"] = df.date.progress_map(lambda d: d.year)\n",
    "df[\"date_month\"] = df.date.progress_map(lambda d: d.month)\n",
    "df[\"date_day\"] = df.date.progress_map(lambda d: d.day)\n",
    "df[\"date_day_of_week\"] = df.date.progress_map(lambda d: d.weekday())\n",
    "df = df.sort_values(by=\"date\").reset_index()\n",
    "df = df[\n",
    "    [\"date_year\", \"date_month\", \"date_day\", \"date_day_of_week\"]\n",
    "    + list(ppd_df.columns[1:-1])\n",
    "    + [\"inflation_rate\", \"interest_rate\", \"price\"]\n",
    "]\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(data_folderpath / \"snapshot-Xy-1NF.zip\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert df is not None\n",
    "except NameError:\n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"[SNAPSHOT] Reloading...\")\n",
    "    pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n",
    "    data_folderpath = pathlib.Path(\"./data\")\n",
    "    df = pd.read_csv(data_folderpath / \"snapshot-Xy-1NF.zip\").astype(\n",
    "        {\"postgroup\": \"category\", \"property_type\": \"category\", \"old_or_new\": \"category\", \"duration\": \"category\"}\n",
    "    )\n",
    "    print(f\" - reloaded from snapshot, {df.shape[0]}\")\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats_continuous = df.select_dtypes(include=\"float\")\n",
    "df_feats_continuous.sample(n=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Tendency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central_tendency = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(df_feats_continuous.mean(), columns=[\"mean\"]).transpose(),\n",
    "        pd.DataFrame(df_feats_continuous.median(), columns=[\"median\"]).transpose(),\n",
    "        pd.DataFrame([df_feats_continuous[c].mode()[0] for c in df_feats_continuous.columns], columns=[\"mode\"])\n",
    "        .set_index(df_feats_continuous.columns)\n",
    "        .transpose(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_central_tendency\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_measures_of_spread = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(df_feats_continuous.var(), columns=[\"var\"]).transpose(),\n",
    "        df_feats_continuous.describe(),\n",
    "        pd.DataFrame(\n",
    "            df_feats_continuous.quantile(0.75) - df_feats_continuous.quantile(0.25), columns=[\"IQR\"]\n",
    "        ).transpose(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "df_measures_of_spread\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_types_of_distros = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(df_feats_continuous.skew(), columns=[\"skew\"]).transpose(),\n",
    "        pd.DataFrame(df_feats_continuous.kurtosis(), columns=[\"kurtosis\"]).transpose(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_types_of_distros\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert df is not None\n",
    "except NameError:\n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"[SNAPSHOT] Reloading...\")\n",
    "    pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n",
    "    data_folderpath = pathlib.Path(\"./data\")\n",
    "    df = pd.read_csv(data_folderpath / \"snapshot-Xy-1NF.zip\").astype(\n",
    "        {\n",
    "            \"postgroup\": \"category\",\n",
    "            \"property_type\": \"category\",\n",
    "            \"old_or_new\": \"category\",\n",
    "            \"duration\": \"category\",\n",
    "            \"price\": \"double\",\n",
    "        }\n",
    "    )\n",
    "    print(f\" - reloaded from snapshot, {df.shape[0]}\")\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats_continuous = df.select_dtypes(include=\"float\")\n",
    "df_feats_continuous.sample(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Continuous Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats_cont_scal_pt = df[list(df_feats_continuous.columns) + [\"property_type\"]]\n",
    "df_feats_cont_scal_pt = df_feats_cont_scal_pt.sample(n=int(df_feats_cont_scal_pt.shape[0] * 0.01))\n",
    "df_feats_cont_scal_pt = df_feats_cont_scal_pt.copy()\n",
    "df_feats_cont_scal_pt[\"inflation_rate\"] *= 100.0\n",
    "df_feats_cont_scal_pt[\"interest_rate\"] *= 100.0\n",
    "df_feats_cont_scal_pt.rename(\n",
    "    columns={\n",
    "        \"inflation_rate\": \"inflation_rate (%)\",\n",
    "        \"interest_rate\": \"interest_rate (%)\",\n",
    "        \"price\": \"price (millions of £)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "sns.pairplot(\n",
    "    data=df_feats_cont_scal_pt,\n",
    "    hue=\"property_type\",\n",
    "    kind=\"scatter\",\n",
    "    diag_kind=\"kde\",\n",
    "    palette=\"viridis\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions for Inflation Rate & Interest Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rate_distributions(ax, df: pd.DataFrame, label: str, color: str):\n",
    "    df = df.copy()\n",
    "    df[\"rate\"] = df[\"rate\"] * 100.0\n",
    "    x = np.linspace(0.0, df[\"rate\"].max(), 100)\n",
    "    df[\"bin\"] = pd.cut(df[\"rate\"], bins=x)\n",
    "    y = list(df.groupby(\"bin\").count()[\"rate\"])\n",
    "    ax.fill_between(x[:-1], 0.0, y, color=color, alpha=0.5)\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter(\"%2.2f%%\"))\n",
    "    intervals = [0.05, 0.5, 0.95]\n",
    "    for interval, quantile in zip(intervals, df.rate.quantile(intervals)):\n",
    "        percentile = f\"P{int(interval*100.)}={round(quantile, 2)}\"\n",
    "        bbox = dict(boxstyle=\"round, pad=0.3\", fc=\"lightgray\", lw=2)\n",
    "        ax.axvline(x=quantile, color=\"blue\")\n",
    "        ax.annotate(\n",
    "            percentile,\n",
    "            xy=(quantile, max(y)),\n",
    "            bbox=bbox,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "    ax.axvline(x=quantile, color=\"blue\")\n",
    "    ax.legend([label], loc=\"lower center\", bbox_to_anchor=(0.5, -0.2))\n",
    "\n",
    "\n",
    "df_daily_means = df.groupby([\"date_year\", \"date_month\", \"date_day\"]).mean(numeric_only=True)\n",
    "df_daily_means_interest = df_daily_means[[\"interest_rate\"]].rename(columns={\"interest_rate\": \"rate\"})\n",
    "df_daily_means_inflation = df_daily_means[[\"inflation_rate\"]].rename(columns={\"inflation_rate\": \"rate\"})\n",
    "_, axes = plt.subplots(ncols=2, nrows=1, figsize=(10, 3))\n",
    "plot_rate_distributions(\n",
    "    ax=axes[0],\n",
    "    df=df_daily_means_interest,\n",
    "    label=\"interest\",\n",
    "    color=\"green\",\n",
    ")\n",
    "plot_rate_distributions(\n",
    "    ax=axes[1],\n",
    "    df=df_daily_means_inflation,\n",
    "    label=\"inflation\",\n",
    "    color=\"red\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Distribution Inflation, Interest & Property Price per Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "def plot_inflation_and_interest(\n",
    "    ax: plt.Axes,\n",
    "    df_mean_by_date: pd.DataFrame,\n",
    "    xlim_left: float,\n",
    "    xlim_right: float,\n",
    "):\n",
    "    df_mean_by_date = df_mean_by_date.reset_index()\n",
    "    x = df_mean_by_date.apply(lambda r: date(int(r.date_year), int(r.date_month), int(r.date_day)), axis=1)\n",
    "    ax.grid(visible=True)\n",
    "    ax.plot(x, df_mean_by_date.interest_rate * 100.0, \"g.-\", alpha=0.7)\n",
    "    ax.plot(x, df_mean_by_date.inflation_rate * 100.0, \"r.-\", alpha=0.7)\n",
    "    ax.set_xlim(left=xlim_left, right=xlim_right)\n",
    "    ax.set_ylabel(\"rates (%)\")\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%2.2f%%\"))\n",
    "    ax.legend([\"interest\", \"inflation\"])\n",
    "\n",
    "\n",
    "def plot_prices_per_property_type(\n",
    "    ax: plt.Axes,\n",
    "    df_mean_by_date_and_pt: pd.DataFrame,\n",
    "):\n",
    "    df_mean_by_date_and_pt = df_mean_by_date_and_pt.reset_index()\n",
    "    ax.grid(visible=True)\n",
    "    ax.yaxis.set_major_formatter(StrMethodFormatter(\"{x:,}\"))\n",
    "    ax.set_ylim(0.0, df_mean_by_date_and_pt.price.quantile(0.95) * 1.2)\n",
    "    ax.set_ylabel(\"property price (£)\")\n",
    "    property_types = sorted(df_mean_by_date_and_pt.property_type.unique())\n",
    "    for ix, property_type in tqdm(list(enumerate(property_types))):\n",
    "        sub_series = df_mean_by_date_and_pt[df_mean_by_date_and_pt.property_type == property_type].copy()\n",
    "        sub_series = sub_series.reset_index().groupby([\"date_year\", \"date_month\"]).mean(numeric_only=True).reset_index()\n",
    "        sub_series = sub_series.fillna(method=\"ffill\")\n",
    "        x = sub_series.apply(lambda r: date(int(r.date_year), int(r.date_month), 1), axis=1)\n",
    "        ax.plot(x, sub_series.price, \"s\", alpha=0.7)\n",
    "        ax.legend(property_types)\n",
    "\n",
    "\n",
    "min_intersecting_date = date(df.date_year.min(), 1, 1)\n",
    "max_intersecting_date = date(df.date_year.max(), 12, 30)\n",
    "_, axes = plt.subplots(nrows=2, figsize=(10, 8), sharex=True)\n",
    "plot_inflation_and_interest(\n",
    "    ax=axes[0],\n",
    "    df_mean_by_date=df.groupby([\"date_year\", \"date_month\", \"date_day\"]).mean(numeric_only=True),\n",
    "    xlim_left=min_intersecting_date,\n",
    "    xlim_right=max_intersecting_date,\n",
    ")\n",
    "plot_prices_per_property_type(\n",
    "    ax=axes[1],\n",
    "    df_mean_by_date_and_pt=df.groupby([\"date_year\", \"date_month\", \"date_day\", \"property_type\"]).mean(numeric_only=True),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('exp-20221228_1617', 42, 0.99)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "r = 42\n",
    "train_size = 0.99\n",
    "experiment_name = f\"exp-{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "(experiment_name, r, train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('results/exp-20221228_1617')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "results_dir = pathlib.Path(f\"./results/{experiment_name}\")\n",
    "shutil.rmtree(results_dir, ignore_errors=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SNAPSHOT] Reloading...\n",
      " - reloaded from snapshot, 4336841\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_year</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_day</th>\n",
       "      <th>date_day_of_week</th>\n",
       "      <th>postgroup</th>\n",
       "      <th>property_type</th>\n",
       "      <th>old_or_new</th>\n",
       "      <th>duration</th>\n",
       "      <th>inflation_rate</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2936038</th>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>SP5</td>\n",
       "      <td>semi-detached</td>\n",
       "      <td>old</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>460,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125388</th>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>B67</td>\n",
       "      <td>terraced</td>\n",
       "      <td>old</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>117,500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508844</th>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>CB6</td>\n",
       "      <td>terraced</td>\n",
       "      <td>new</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>255,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413188</th>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>TN17</td>\n",
       "      <td>detached</td>\n",
       "      <td>old</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>827,500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535232</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>NP4</td>\n",
       "      <td>terraced</td>\n",
       "      <td>old</td>\n",
       "      <td>freehold</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.001</td>\n",
       "      <td>170,000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_year  date_month  date_day  date_day_of_week postgroup  \\\n",
       "2936038       2021           3        11                 3       SP5   \n",
       "125388        2018           2        27                 1       B67   \n",
       "1508844       2019           8         9                 4       CB6   \n",
       "3413188       2021           7        27                 1      TN17   \n",
       "3535232       2021           9        14                 1       NP4   \n",
       "\n",
       "         property_type old_or_new  duration  inflation_rate  interest_rate  \\\n",
       "2936038  semi-detached        old  freehold           0.010          0.001   \n",
       "125388        terraced        old  freehold           0.025          0.005   \n",
       "1508844       terraced        new  freehold           0.017          0.007   \n",
       "3413188       detached        old  freehold           0.021          0.001   \n",
       "3535232       terraced        old  freehold           0.029          0.001   \n",
       "\n",
       "              price  \n",
       "2936038 460,000.000  \n",
       "125388  117,500.000  \n",
       "1508844 255,000.000  \n",
       "3413188 827,500.000  \n",
       "3535232 170,000.000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    assert df is not None\n",
    "except NameError:\n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"[SNAPSHOT] Reloading...\")\n",
    "    pd.set_option(\"display.float_format\", lambda x: \"{:,.3f}\".format(x))\n",
    "    data_folderpath = pathlib.Path(\"./data\")\n",
    "    df = pd.read_csv(data_folderpath / \"snapshot-Xy-1NF.zip\").astype(\n",
    "        {\n",
    "            \"postgroup\": \"category\",\n",
    "            \"property_type\": \"category\",\n",
    "            \"old_or_new\": \"category\",\n",
    "            \"duration\": \"category\",\n",
    "            \"price\": \"double\",\n",
    "        }\n",
    "    )\n",
    "    print(f\" - reloaded from snapshot, {df.shape[0]}\")\n",
    "df.sample(n=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matrix & Targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[df.columns[:-1]], df[df.columns[-1]]\n",
    "del df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out Test-Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   split      |X|      |y|       %\n",
      "0   full  4336841  4336841  101.0%\n",
      "1  train  4293472  4293472   99.0%\n",
      "2   test    43369    43369    1.0%\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def produce_split_summary(\n",
    "    X_split: pd.DataFrame, y_split: pd.DataFrame, name: str, total: int\n",
    ") -> Tuple[str, int, int, str]:\n",
    "    return (\n",
    "        name,\n",
    "        X_split.shape[0],\n",
    "        y_split.shape[0],\n",
    "        \"{:.1f}%\".format(100.0 * X_split.shape[0] / total),\n",
    "    )\n",
    "\n",
    "\n",
    "X1, X2, y1, y2 = train_test_split(X, y, train_size=train_size, random_state=r)\n",
    "print(pd.DataFrame(\n",
    "    [\n",
    "        produce_split_summary(X, y, \"full\", total=X1.shape[0]),\n",
    "        produce_split_summary(X1, y1, \"train\", total=X.shape[0]),\n",
    "        produce_split_summary(X2, y2, \"test\", total=X.shape[0]),\n",
    "    ],\n",
    "    columns=[\"split\", \"|X|\", \"|y|\", \"%\"],\n",
    "))\n",
    "del X, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[svd_polynomial_regression]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m model_name, model_instance \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     83\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m     model_instance\u001b[39m.\u001b[39;49mfit(X1, y1)\n\u001b[1;32m     85\u001b[0m     y1_pred \u001b[39m=\u001b[39m model_instance\u001b[39m.\u001b[39mpredict(X1)\n\u001b[1;32m     86\u001b[0m     y2_pred \u001b[39m=\u001b[39m model_instance\u001b[39m.\u001b[39mpredict(X2)\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/sklearn/pipeline.py:402\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \n\u001b[1;32m    378\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 402\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    403\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/sklearn/pipeline.py:360\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    358\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    359\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    361\u001b[0m     cloned_transformer,\n\u001b[1;32m    362\u001b[0m     X,\n\u001b[1;32m    363\u001b[0m     y,\n\u001b[1;32m    364\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    365\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    366\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    367\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    369\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/sklearn/pipeline.py:894\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    893\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 894\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    895\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:750\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_output(Xs)\n\u001b[1;32m    748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_record_output_indices(Xs)\n\u001b[0;32m--> 750\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hstack(\u001b[39mlist\u001b[39;49m(Xs))\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:839\u001b[0m, in \u001b[0;36mColumnTransformer._hstack\u001b[0;34m(self, Xs)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    834\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    835\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFor a sparse output, all columns should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    836\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbe a numeric or convertible to a numeric.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    837\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m     \u001b[39mreturn\u001b[39;00m sparse\u001b[39m.\u001b[39;49mhstack(converted_Xs)\u001b[39m.\u001b[39;49mtocsr()\n\u001b[1;32m    840\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m     Xs \u001b[39m=\u001b[39m [f\u001b[39m.\u001b[39mtoarray() \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(f) \u001b[39melse\u001b[39;00m f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m Xs]\n",
      "File \u001b[0;32m~/Workspaces/hudsonmendes-estudos/cm3005-dsc/venv/lib/python3.8/site-packages/scipy/sparse/_coo.py:404\u001b[0m, in \u001b[0;36mcoo_matrix.tocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    401\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(col, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[1;32m    402\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty_like(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, dtype\u001b[39m=\u001b[39mupcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m--> 404\u001b[0m coo_tocsr(M, N, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnnz, row, col, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    405\u001b[0m           indptr, indices, data)\n\u001b[1;32m    407\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_csr_container((data, indices, indptr), shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_canonical_format:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "\n",
    "def make_sine_cycle_encoder(period: int = 1) -> float:\n",
    "    assert period != 0\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use\n",
    "def make_df_column_transformer(scale_non_categorical: bool = False):\n",
    "    categorical_selector = make_column_selector(dtype_include=\"category\")\n",
    "    non_categorical_selector = make_column_selector(dtype_exclude=\"category\")\n",
    "    one_hot = OneHotEncoder(sparse_output=True, handle_unknown=\"ignore\")\n",
    "    cycle_sine_12 = make_sine_cycle_encoder(period=12)\n",
    "    cycle_sine_31 = make_sine_cycle_encoder(period=31)\n",
    "    cycle_sine_6 = make_sine_cycle_encoder(period=6)\n",
    "    steps = [\n",
    "        (one_hot, categorical_selector),\n",
    "        (cycle_sine_12, [\"date_month\"]),\n",
    "        (cycle_sine_31, [\"date_day\"]),\n",
    "        (cycle_sine_6, [\"date_day_of_week\"]),\n",
    "    ]\n",
    "    if scale_non_categorical:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        steps.append((scaler, non_categorical_selector))\n",
    "    return make_column_transformer(*steps, remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "def make_svd_polynomial_regression_model(\n",
    "    random_state: int,\n",
    "    svd_dims: int = 1,\n",
    "    polynomial_degree: int = 1,\n",
    "    fit_intercept: bool = False,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    return make_pipeline(\n",
    "        make_df_column_transformer(),\n",
    "        TruncatedSVD(n_components=svd_dims, algorithm=\"arpack\", random_state=r),\n",
    "        PolynomialFeatures(degree=polynomial_degree, include_bias=False),\n",
    "        LinearRegression(fit_intercept=fit_intercept),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def make_mlp_regression_model(\n",
    "    random_state: int,\n",
    "    embedding_layer_dims: int = None,\n",
    "    residual_layer_dims: int = 1,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    hidden_layer_sizes = []\n",
    "    if embedding_layer_dims:\n",
    "        hidden_layer_sizes.append(embedding_layer_dims)\n",
    "    if residual_layer_dims:\n",
    "        hidden_layer_sizes.append(residual_layer_dims)\n",
    "    return make_pipeline(\n",
    "        make_df_column_transformer(scale_non_categorical=True),\n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=tuple(hidden_layer_sizes),\n",
    "            solver=\"adam\",\n",
    "            alpha=0.0001,\n",
    "            max_iter=1,\n",
    "            early_stopping=True,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"svd_polynomial_regression\": make_svd_polynomial_regression_model(random_state=r, verbose=True),\n",
    "    \"mlp_regression\": make_mlp_regression_model(random_state=r, verbose=True),\n",
    "}\n",
    "for model_name, model_instance in models.items():\n",
    "    print(f\"[{model_name}]\")\n",
    "    model_instance.fit(X1, y1)\n",
    "    y1_pred = model_instance.predict(X1)\n",
    "    y2_pred = model_instance.predict(X2)\n",
    "    model_metrics = {\n",
    "        \"train\": {\n",
    "            \"mae\": mean_absolute_error(y1_pred, y1),\n",
    "            \"mape\": mean_absolute_percentage_error(y1_pred, y1),\n",
    "            \"r2\": mean_absolute_error(y1_pred, y1)\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"mae\": mean_absolute_error(y2_pred, y2),\n",
    "            \"mape\": mean_absolute_percentage_error(y2_pred, y2),\n",
    "            \"r2\": mean_absolute_error(y2_pred, y2)\n",
    "        }\n",
    "    }\n",
    "    print(pd.DataFrame.from_dict(model_metrics))\n",
    "del models\n",
    "del model_metrics\n",
    "del y1_pred, y2_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "\n",
    "def make_grid(\n",
    "    model: Pipeline,\n",
    "    param_grid: Dict[str, Any],\n",
    ") -> GridSearchCV:\n",
    "    return GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_absolute_percentage_error\",\n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "\n",
    "def export_grid_results(\n",
    "    results_dir: pathlib.Path,\n",
    "    experiment_name: str,\n",
    "    df_best_params: pd.DataFrame,\n",
    "    df_metrics: pd.DataFrame,\n",
    "    df_search: pd.DataFrame,\n",
    "):\n",
    "    experiment_dir = results_dir / experiment_name\n",
    "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_best_params.to_csv(experiment_dir / \"best_params.csv\", header=False)\n",
    "    df_metrics.to_csv(experiment_dir / \"best_metrics.csv\", header=False)\n",
    "    df_search.to_csv(experiment_dir / \"grid_search.csv\", index=False)\n",
    "\n",
    "\n",
    "def compile_best_metrics(\n",
    "    grid: GridSearchCV,\n",
    "    training_data: Tuple[np.array, np.array],\n",
    "    test_data: Tuple[np.array, np.array],\n",
    ") -> pd.DataFrame:\n",
    "    X_train, y_true_train = training_data\n",
    "    X_test, y_true_test = test_data\n",
    "    y_pred_train = grid.predict(X_train)\n",
    "    y_pred_test = grid.predict(X_test)\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"train\": {\n",
    "                \"mae\": mean_absolute_error(y_pred_train, y_true_train),\n",
    "                \"mape\": mean_absolute_percentage_error(y_pred_train, y_true_train),\n",
    "                \"r2\": r2_score(y_pred_train, y_true_train),\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"mae\": mean_absolute_error(y_pred_test, y_true_test),\n",
    "                \"mape\": mean_absolute_percentage_error(y_pred_test, y_true_test),\n",
    "                \"r2\": r2_score(y_pred_test, y_true_test),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def compile_search_data(cv_results: Dict[str, Any]):\n",
    "    df_search = pd.DataFrame.from_dict(cv_results)\n",
    "    df_search = df_search.sort_values(by=\"rank_test_score\").reset_index(drop=True)\n",
    "    df_search_cols_key = [\"rank_test_score\"]\n",
    "    df_search_cols_key.extend([c for c in df_search.columns.tolist() if c.startswith(\"param_\")])\n",
    "    df_search_cols_key.extend([\"mean_test_score\", \"std_test_score\"])\n",
    "    df_results_cols_rest = [c for c in df_search.columns.tolist() if c not in df_search_cols_key]\n",
    "    df_results_cols_rest.remove(\"params\")\n",
    "    df_search = df_search[df_search_cols_key + df_results_cols_rest]\n",
    "\n",
    "\n",
    "def run_grid_experiment(\n",
    "    experiment: str,\n",
    "    model: Pipeline,\n",
    "    training_data: Tuple[np.array, np.array],\n",
    "    test_data: Tuple[np.array, np.array],\n",
    "    param_grid: Dict[str, Any],\n",
    "    results_dir: pathlib.Path,\n",
    "):\n",
    "    print(f\"[{experiment}]\")\n",
    "    grid = make_grid(model, param_grid)\n",
    "    X_train, y_train = training_data\n",
    "    grid.fit(X_train, y_train)\n",
    "    df_best_params = pd.DataFrame.from_dict(grid.best_params_, orient=\"index\")\n",
    "    df_best_metrics = compile_best_metrics(grid, training_data, test_data)\n",
    "    df_search_data = compile_search_data(grid.cv_results_)\n",
    "    return export_grid_results(\n",
    "        grid,\n",
    "        results_dir,\n",
    "        results=[\n",
    "            (\"best-params\", df_best_params),\n",
    "            (\"best-metrics\", df_best_metrics),\n",
    "            (\"search-data\", df_search_data),\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Polynomial Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_svd_polynomial_regression_model(random_state=r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"svd_polynomial_regression_tinny\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"truncatedsvd__n_components\": [1],\n",
    "        \"polynomialfeatures__degree\": [1],\n",
    "        \"linearregression__fit_intercept\": [False],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"svd_polynomial_regression_sm\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"truncatedsvd__n_components\": [3],\n",
    "        \"polynomialfeatures__degree\": [1, 2, 3],\n",
    "        \"linearregression__fit_intercept\": [True, False],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"svd_polynomial_regression_md\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"truncatedsvd__n_components\": [4, 8],\n",
    "        \"polynomialfeatures__degree\": [3],\n",
    "        \"linearregression__fit_intercept\": [True, False],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"svd_polynomial_regression_lg\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"truncatedsvd__n_components\": [4, 8],\n",
    "        \"polynomialfeatures__degree\": [4],\n",
    "        \"linearregression__fit_intercept\": [True, False],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: MLP Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_mlp_regression_model(random_state=r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"mlp_regression_emb_only\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"mlpregressor__hidden_layer_sizes\": [(2,), (4,), (8,)],\n",
    "        \"mlpregressor__solver\": [\"adam\", \"lbfgs\"],\n",
    "        \"mlpregressor__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "        \"mlpregressor__max_iter\": [25],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"mlp_regression_sm\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"mlpregressor__hidden_layer_sizes\": [(2, 2), (2, 4)],\n",
    "        \"mlpregressor__solver\": [\"adam\", \"lbfgs\"],\n",
    "        \"mlpregressor__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "        \"mlpregressor__max_iter\": [25],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"mlp_regression_sm\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"mlpregressor__hidden_layer_sizes\": [(4, 4), (4, 8)],\n",
    "        \"mlpregressor__solver\": [\"adam\", \"lbfgs\"],\n",
    "        \"mlpregressor__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "        \"mlpregressor__max_iter\": [25],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_experiment(\n",
    "    experiment=f\"mlp_regression_lg\",\n",
    "    model=model,\n",
    "    training_data=(X1, y1),\n",
    "    test_data=(X2, y2),\n",
    "    param_grid={\n",
    "        \"mlpregressor__hidden_layer_sizes\": [(8, 8), (8, 16)],\n",
    "        \"mlpregressor__solver\": [\"adam\", \"lbfgs\"],\n",
    "        \"mlpregressor__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "        \"mlpregressor__max_iter\": [25],\n",
    "    },\n",
    "    results_dir=results_dir,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Conclusions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing remarks/statements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff2a5bf76a5a6a6e16a446abb6a1160221241a1606404c7f1c7e55bf8cf0847f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
